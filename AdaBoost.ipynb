{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24fd0b8-aa3b-4246-bff2-4c6a1d27ae4c",
   "metadata": {},
   "source": [
    "# Modele boostingowe\n",
    "Na poprzednich zajęciach wykorzystywaliśmy algorytymy losowego lasu oraz ekstremalnego losowego lasu, które opierały się na założeniu, że bardzo dobry model można uzyskać poprzez uśrednienie wyników wielu dobrych modeli (wisdom of crowd). Modele losowego lasu oraz ekstremalnego losowego lasu są przykładami - *ensembple bagging models*. \n",
    "\n",
    "> Ensemble bagging models, czyli modele zespołowe z baggingiem, to rodzaj metod zespołowego uczenia maszynowego, które mają na celu poprawę dokładności i odporności modeli predykcyjnych. \"Bagging\" to skrót od \"Bootstrap Aggregating\" i polega na tworzeniu wielu wersji zbioru danych treningowych za pomocą próbkowania bootstrapowego (losowego próbkowania ze zwracaniem) oraz trenowaniu modelu na każdym z tych zbiorów. Końcowa prognoza jest uzyskiwana przez uśrednianie prognoz (dla zadań regresji) lub poprzez głosowanie większościowe (dla zadań klasyfikacji) poszczególnych modeli.\n",
    "\n",
    "Teraz zajmiemy się modelami boostingowymi. \n",
    "\n",
    "> Boosting (wzmacnianie) to klasa algorytmów zespołowych, w której modele są dodawane sekwencyjnie, tak aby późniejsze modele w sekwencji korygowały prognozy dokonane przez wcześniejsze modele w tej sekwencji.\n",
    "\n",
    "# AdaBoost\n",
    "Modelem podstawowym z którego wyewoluowały modele boostingowe jest AdaBoost. \n",
    "\n",
    "## Algorytm – AdaBoost (Adaptive Boosting)\n",
    "1. **Przydzielanie jednolitych wag**: - Każdej próbce treningowej przypisywane są jednolite wagi. Mówiąc prościej, każda próbka treningowa ma taką samą szansę pojawienia się podczas pierwszego trenowania słabego modelu.\n",
    "2. **Trenowanie słabego modelu**: - Trenuje się słaby model (base_estimator w scikit-learn) na losowym wyborze (z zastępowaniem) zbioru treningowego.\n",
    "3. **Obliczanie błędu predykcji słabego modelu**: - Obliczany jest błąd predykcji słabego modelu na (całym) zbiorze treningowym, używając funkcji straty, takiej jak błąd kwadratowy lub błąd absolutny (loss w scikit-learn).\n",
    "4. **Obliczanie pewności słabego modelu**: - Obliczana jest pewność słabego modelu na podstawie jego średniego błędu predykcji (obliczonego w kroku 3). Im mniejszy błąd, tym większa pewność.\n",
    "5. **Aktualizacja wag zbioru treningowego**: - Wagi zbioru treningowego są aktualizowane na podstawie błędów słabego modelu, jego pewności i współczynnika uczenia (learning_rate w scikit-learn). Im niższy współczynnik uczenia, tym mniejszy wpływ na wagi zbioru treningowego. Innymi słowy, niski współczynnik uczenia spowalnia aktualizację wag, wymagając tym samym większej liczby drzew do znaczącej zmiany rozkładu zbioru treningowego. Próbki danych z największymi błędami predykcji otrzymują większą wagę, podczas gdy próbki danych z najmniejszymi błędami predykcji mają zmniejszaną wagę.\n",
    "6. **Powtarzanie kroków 2-5**:\n",
    " - Kroki 2-5 są powtarzane, aż do dopasowania określonej liczby słabych modeli (n_estimators w scikit-learn).\n",
    "\n",
    "**Końcowa prognoza modelu zespołowego**:\n",
    "- Końcowa prognoza modelu zespołowego jest podawana jako ważona mediana prognoz wszystkich słabych uczących się modeli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434642c6-7e6f-48a6-8986-0dce46d92ae4",
   "metadata": {},
   "source": [
    "# Hiperparametry algorytmu AdaBoost\n",
    "Parametry algorytmu AdaBoost:\n",
    "- **base_estimator**: Jest to weak learner się, którego chcemy wzmocnić. Zazwyczaj używamy drzewa o określonej maksymalnej głębokości.\n",
    "- **n_estimators**: Liczba wygenerowanych weak learners.\n",
    "- **learning_rate**: Współczynnik, który zmniejsza ogólny wpływ każdego słabego modelu na wagi zbioru treningowego.\n",
    "- **loss**: Funkcja straty używana przy aktualizacji przydziału wag dla każdej próbki przed ponownym przetasowaniem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc3b7d-4fb4-4463-b243-bc19c531b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane (w oparciu o kod z porpzednich zajęć)\n",
    "\n",
    "# Wykorzystując zbiór z poprzednich zajęć ToyotaCorolla zbuduj prosty model AdaBoost\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8),\n",
    "                        n_estimators=100, learning_rate=0.25, loss='square')\n",
    "ada = ada.fit(X_train, Y_train)\n",
    "\n",
    "# Oblicz parametry modelu wykorzystując funkcję KPI i porównaj z drzewami\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c7b70-2044-4af8-a821-1a957f81349a",
   "metadata": {},
   "source": [
    "# Optymalizacja modelu\n",
    "Funkcje automatyzujące przeszukiwanie nie są w stanie automatycznie zoptymalizować max_depth dlatego trzeba wykorzystać funkcję for. Dokonaj optymalizacj parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5a437-4240-40b2-8657-6b01ecd972a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [100]\n",
    "learning_rate = [0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "loss = ['square', 'exponential', 'linear']\n",
    "param_dist = {\n",
    "    # 'n_estimators': n_estimators,\n",
    "    'learning_rate': learning_rate,\n",
    "    'loss': loss\n",
    "}\n",
    "\n",
    "# [ToDo] Napisz optymalizację \n",
    "# dla max_depth w zakresie od 2 do 18 ze skokiem dwa\n",
    "# uruchom optymalizację wykorzystująć randomsearch\n",
    "# dodaj najlepszy wynik dla danego max_depth do dataframe - best_parameters\n",
    "# znajdź najlepszą wartość parametru Score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
