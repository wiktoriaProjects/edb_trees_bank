{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4a8b26-b574-4880-84fe-423f953f38d6",
   "metadata": {},
   "source": [
    "# Drzewa i lasy\n",
    "W czasie tego ćwiczenia będziemy przewidywali ceny samochodu Toyota Corolla, z wykorzystaniem 3 metod:\n",
    "\n",
    "- drzew (1964)\n",
    "- losowego lasu (1995)\n",
    "- Ekstremalnie losowego lasu (2006)\n",
    "\n",
    "## Wczytanie i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d81d9b7-05cb-41ee-90a6-4c6560447465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\424423\\AppData\\Local\\Temp\\ipykernel_3616\\3724516347.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Wczytanie danych\n",
    "toyota_df = pd.read_csv('ToyotaCorolla.csv')\n",
    "toyota_df = toyota_df.rename(columns={'age_08_04': 'age', 'quarterly_tax': 'tax'})\n",
    "predictors = ['age', 'km', 'fuel_type', 'hp', 'met_color', 'automatic', 'cc', \n",
    "              'doors', 'tax', 'weight']\n",
    "outcome = 'price'\n",
    "X = pd.get_dummies(toyota_df[predictors], drop_first=True)\n",
    "Y = toyota_df[outcome]\n",
    "\n",
    "# [ToDo] Podziel próbkę na testową i uczącą w stosunku 40/60\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d597a9a-ad43-4166-8dcd-a3c24306abd6",
   "metadata": {},
   "source": [
    "# Metoda 1 - drzewo regresyjne\n",
    "Wykorzystując więdze z poprzednich zajęć zbuduj drzewo regresyjne. \n",
    "\n",
    "# Algorytm znalezienia najlepszego rozwiazania:\n",
    "1. Wykorzystując Grid Search oraz podany `param_grid` znajdź najlepszą kombinację parametrów.\n",
    "2. Wyświetl najlepsze drzewo.\n",
    "3. Popraw działanie algorytmu poprzez random search budując `param_grid1` gdzie każdy z parametrów jest listą: `max_depth` oraz `min_sample_split` - plus minus 2 ze skokiem 1, 'min_impurity_decrease' - plus minus 0.002 ze skokiem 0.001.\n",
    "\n",
    "Porównaj Otrzymane drzewa pod kątem RMSE oraz struktury.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce0f5f7-dc0f-45cb-aba2-6178afd788e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najlepsze parametry drzewa grid search {'max_depth': 15, 'min_impurity_decrease': 0, 'min_samples_split': 10}\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'decision_tree' parameter of export_text must be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got RandomForestRegressor(n_estimators={'max_depth': 15, 'min_impurity_decrease': 0,\n                                    'min_samples_split': 10}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     21\u001b[0m regTree \u001b[38;5;241m=\u001b[39m RandomForestRegressor(best_params)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# [ToDo] Narysować najlepsze drzewo\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#best_params = tree_gs.best_params_\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#best_tree = RandomForestRegressor(best_params)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# [ToDO] Reprezentacja tekstowa drzewa\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m text_representation \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregTree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_representation)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# [ToDO] ile wynosi MPE, MAE i RMSE - wykorzystaj funkcję KPI z poprzednich zajęć - próbka ucząca i testowa\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# [ToDo] Parametry drugiego etapu dopasowania\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# [ToDo] Wyznaczyć RMSE dla próbek uczącej i testowej\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    204\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39;49mfunc\u001b[39m.\u001b[39;49m\u001b[39m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'decision_tree' parameter of export_text must be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got RandomForestRegressor(n_estimators={'max_depth': 15, 'min_impurity_decrease': 0,\n                                    'min_samples_split': 10}) instead."
     ]
    }
   ],
   "source": [
    "# Zbuduj drzweo \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "forest = RandomForestRegressor(bootstrap= True, max_samples =0.95)\n",
    "\n",
    "# Parametry optymalizacji\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25], \n",
    "    'min_impurity_decrease': [0, 0.001, 0.005, 0.01], \n",
    "    'min_samples_split': [10, 20, 30, 40, 50], \n",
    "}\n",
    "# [Komentarz] Wykorzystujemy nowy paramater min_impurity_decrease\n",
    "\n",
    "# [ToDo] Wykorzystać GridSearch to znalezienia najlepszego drzewa\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "tree_gs = GridSearchCV(forest, param_grid, n_jobs=-1, cv=3, verbose=0)\n",
    "fc=tree_gs.fit(train_X,train_y)\n",
    "print('Najlepsze parametry drzewa grid search',tree_gs.best_params_)\n",
    "best_params = tree_gs.best_params_\n",
    "regTree = RandomForestRegressor(best_params)\n",
    "\n",
    "# [ToDo] Narysować najlepsze drzewo\n",
    "#best_params = tree_gs.best_params_\n",
    "#best_tree = RandomForestRegressor(best_params)\n",
    "#fit_forest = best_tree.fit(train_X, train_y)\n",
    "#tree.plot_tree(fit_forest, feature_names=X.columns, filled=True)\n",
    "\n",
    "# [ToDO] Reprezentacja tekstowa drzewa\n",
    "text_representation = tree.export_text(regTree, feature_names=train_X.columns)\n",
    "print(text_representation)\n",
    "\n",
    "# [ToDO] ile wynosi MPE, MAE i RMSE - wykorzystaj funkcję KPI z poprzednich zajęć - próbka ucząca i testowa\n",
    "\n",
    "# [ToDo] Parametry drugiego etapu dopasowania\n",
    "\n",
    "# [ToDo] Wykorzystać RandomSearch do poprawy dopasoawnia\n",
    "\n",
    "# [ToDo] Narysować drzewo\n",
    "\n",
    "# [ToDo] Wyznaczyć RMSE dla próbek uczącej i testowej\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff9f7c-852c-4243-ade2-3784083ca789",
   "metadata": {},
   "source": [
    "# Metoda 2 - losowy las\n",
    "Losowy las wykorzystuje dwie istotne koncepcje: **the wisdom of crowd** oraz **ensemble model**.\n",
    "- *Mądrość tłumów* - Idea ta wyjaśnia, że średnia opinia grupy ludzi będzie bardziej precyzyjna (średnio) niż opinia pojedynczego członka tej grupy.\n",
    "- *Model zespołowy* - Model zespołowy to (meta)model składający się z wielu podmodeli.\n",
    "\n",
    "## Jak wykorzystać Wisdom of Crowd w praktyce?\n",
    "Generalnie są 3 pomysły jak wykorzystać Wisom of Crowd:\n",
    "\n",
    "*Uczyć różnymi danymi*\n",
    "**Bootstrap** - Każde drzewo otrzyma losowy wybór z początkowego zestawu danych treningowych. Ten losowy wybór jest dokonywany z zastępowaniem. W praktyce oznacza to, że te same punkty danych mogą pojawić się wielokrotnie w zestawie treningowym używanym przez każde drzewo, a niektóre inne punkty danych mogą po prostu nie być użyte.\n",
    "**Ograniczenie Liczby Próbek** - ograniczenie ilości danych przekazywanych do każdego drzewa. W ten sposób ograniczasz ich trening, ale sprawiasz, że każde drzewo jest trochę inne. Liczba próbek może być ustawiona w scikit-learn za pomocą parametru `max_samples`. Zwykle ustawia się współczynnik (na przykład `0.9`), tak aby zachować (losową) część początkowego zestawu treningowego.\n",
    "\n",
    "*Ograniczyć liczbę atrybutów z których może powstać drzewo* \n",
    "\n",
    "Parametr `max_features` w losowym lesie określa maksymalną liczbę cech rozważanych przy podziale w każdym węźle drzewa. Ustawienie tego parametru na mniejszą wartość sprawia, że każde drzewo jest bardziej zróżnicowane i redukuje korelację między drzewami, co zazwyczaj zwiększa ogólną wydajność modelu.\n",
    "\n",
    "\n",
    "## Parametry do losowego lasu\n",
    "Uruchamiająć losowy las następujące parametry uznajemy za istotne.\n",
    "\n",
    "1. **bootstrap (domyślnie: true)**  Ten parametr określa, czy las losowy będzie korzystał z próbkowania bootstrap. Pozostawić tą wartość na *true*.\n",
    "\n",
    "2. **max_depth (domyślnie: None)**  Podobnie jak w przypadku zwykłego drzewa, jest to maksymalna głębokość każdego drzewa (tj. maksymalna liczba kolejnych pytań tak/nie). Ustawmy ją na 7.\n",
    "\n",
    "3. **max_features (domyślnie: 'auto')**  Jest to maksymalna liczba cech, z których drzewo może wybierać podczas dzielenia węzła. Zbiór dostępnych cech do wyboru losowo zmienia się w każdym węźle. Im niższa wartość max_features, tym większe różnice między drzewami lasu (co prowadzi do lepszego lasu), ale tym bardziej ograniczone jest każde drzewo (co skutkuje niższą dokładnością poszczególnych drzew). \n",
    "\n",
    "4. **min_samples_leaf (domyślnie: 1)**  \n",
    "   Podobnie jak w przypadku zwykłego drzewa, jest to minimalna liczba próbek, które musi zawierać każdy liść. Niska wartość pozwoli drzewu bardziej się rozwinąć, co najprawdopodobniej zwiększy dokładność na zbiorze treningowym, ale za cenę ryzyka przeuczenia.\n",
    "\n",
    "5. **min_samples_split (domyślnie: 2)**  Podobnie jak w przypadku zwykłego drzewa, jest to minimalna liczba próbek, jakie musi zawierać węzeł, aby został podzielony. Podobnie jak w przypadku min_samples_leaf, niska wartość najprawdopodobniej zwiększy dokładność na zbiorze treningowym, ale może nie poprawić zbioru testowego, a nawet mu zaszkodzić.\n",
    "6. **n_estimators (domyślnie: 100)**   liczba drzew w lesie. Im więcej, tym lepiej - kosztem dłuższego czasu obliczeń. W pewnym momencie dodatkowy czas działania nie będzie wart minimalnej poprawy.\n",
    "\n",
    "8. **criterion (domyślnie: 'mse')**  Jest to wskaźnik, który algorytm będzie minimalizować. Wybierz 'mse', aby zoptymalizować MSE, lub 'mae' dla MAE. lgorytm tworzenia lasu losowego będzie potrzebował znacznie więcej czasu na optymalizację dla MAE niż dla MSE.\n",
    "\n",
    "# Algorytm znalezienia najlepszego rozwiazania:\n",
    "1. Wykorzystując Grid Search oraz podany `param_grid` znajdź najlepszą kombinację parametrów.\n",
    "2. Wyświetl najlepsze drzewo.\n",
    "3. Popraw działanie algorytmu poprzez random search budując `param_grid1` gdzie każdy z parametrów jest listą: `max_depth` oraz `min_sample_split` - plus minus 2 ze skokiem 1, 'min_impurity_decrease' - plus minus 0.002 ze skokiem 0.001.\n",
    "\n",
    "Porównaj Otrzymane drzewa pod kątem RMSE oraz struktury.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6671f0d-0fe6-4d61-8853-ed361b51c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Przykład działania estymatora - tego kodu nie uruchamiamy ;)\n",
    "forest = RandomForestRegressor(n_jobs=1, n_estimators=30)\n",
    "\n",
    "# forest.fit(X_train,Y_train)\n",
    "\n",
    "# Parametry do optymalizacji\n",
    "max_depth = list(range(5,11)) + [None]\n",
    "min_samples_split = range(5,20)\n",
    "min_samples_leaf = range(2,15)\n",
    "max_features = range(3,8)\n",
    "bootstrap = [True] \n",
    "max_samples = [.7,.8,.9,.95,1]\n",
    "\n",
    "param_dist = {'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features,\n",
    "              'bootstrap': bootstrap,\n",
    "              'max_samples': max_samples}\n",
    "\n",
    "# [ToDo] Dokonaj optymalizacji parametrów z wykorzystaniem random Search. Parametry do Random Search\n",
    "# cv = 6, verbose = 2, n_iter = 400, scoring='neg_mean_absolute_error'\n",
    "# forest = RandomForestRegressor(n_jobs=1, n_estimators=30)\n",
    "\n",
    "# [ToDo] Wyznacz błąd dla najlepszego modelu - czy zwiększenie n_estimators do 200 poprawia wynik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89c1bb-be08-4e81-8598-8bde1c799729",
   "metadata": {},
   "source": [
    "# Interpretacja modelu\n",
    "Do interpretacji modelu wykorzystujemy feature_importance (znaczeni cech). W bibliotece sci_learn znaczenie cechy to redukcja, którą wnosi do celu, który algorytm stara się zminimalizować. W naszym przypadku chcemy obniżyć MAE (średni błąd absolutny). Zatem znaczenie cechy mierzone jest jako dokładność prognozy, którą wnosi każda cecha. Znaczenie cech jest następnie normalizowane (tzn. znaczenie każdej cechy jest skalowane tak, aby suma wszystkich znaczeń cech wynosiła 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab2ce2-90e1-4b18-9c8f-dbe3a502a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Todo] Przerób kod aby uzyskać wykres ważności cech\n",
    "#cols = X_train.shape[1]  # number of columns in our training sets\n",
    "# features = [f'M-{cols-col}' for col in range(cols)]\n",
    "#data = forest.feature_importances_.reshape(-1,1)\n",
    "#imp = pd.DataFrame(data=data, index=features, columns=['Forest'])\n",
    "# imp.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682fb70-734e-4562-86de-748f11dc96fb",
   "metadata": {},
   "source": [
    "# Ekstremalnie losowy las\n",
    "Pomysł na losowy las polegał na tym, że moglibyśmy uzyskać lepszą dokładność prognozy, biorąc średnią przewidywań wielu różnych drzew. W 2006 roku belgijscy badacze Pierre Geurts, Damien Ernst i Louis Wehenkel wprowadzili trzeci pomysł, aby jeszcze bardziej zwiększyć różnice między każdym drzewem. Na każdym węźle algorytm teraz losowo wybiera punkt podziału dla każdej cechy, a następnie wybiera najlepszy podział spośród nich. Choć to jak działa Extremely Randomized Trees (ETR) dziala, czyli losowo wybiera punkty podziału, wydaje się nieintuicyjne, to jednak zwiększa to jeszcze bardziej różnice między każdym drzewem w ETR, co skutkuje lepszą ogólną dokładnością. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb03ee-e888-4ce7-bef2-c864f64859b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import ExtraTreesRegressor\n",
    "#ETR = ExtraTreesRegressor(n_jobs=-1, n_estimators=200,\n",
    "                          min_samples_split=15, min_samples_leaf=4, max_samples=0.95,\n",
    "                          max_features=4, max_depth=8, bootstrap=True)\n",
    "#ETR.fit(X_train,Y_train)\n",
    "\n",
    "# Macież parametrów # Parametry do optymalizacji\n",
    "max_depth = list(range(5,11)) + [None]\n",
    "min_samples_split = range(5,20)\n",
    "min_samples_leaf = range(2,15)\n",
    "max_features = range(3,8)\n",
    "bootstrap = [True] \n",
    "max_samples = [.7,.8,.9,.95,1]\n",
    "\n",
    "param_dist = {'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features,\n",
    "              'bootstrap': bootstrap,\n",
    "              'max_samples': max_samples}\n",
    "\n",
    "# [ToDo] Dokonaj optymalizacji parametrów z wykorzystaniem random Search. Parametry do Random Search\n",
    "# cv = 6, verbose = 2, n_iter = 400, scoring='neg_mean_absolute_error'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9240d949b7e875368571ba59acc67192d2efbcc4561b3c6f94c83d7858e18732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
